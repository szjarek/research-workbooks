{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DisasterTweetsClassification.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMLiWytJ0gnYWmCZqQQKIzW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"w3y3qLlcMiVw","executionInfo":{"status":"ok","timestamp":1616163472889,"user_tz":-60,"elapsed":574,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["import os\n","import sys\n","import pandas as pd\n","from datetime import datetime\n","\n","from google.colab import drive"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVv_i1iHMjOi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616163474615,"user_tz":-60,"elapsed":2297,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"02fa5b0b-fdef-4bb2-d683-a06fb71cea90"},"source":["drive.mount('/content/gdrive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PtmlVcbwMuqH","executionInfo":{"status":"ok","timestamp":1616163474616,"user_tz":-60,"elapsed":2296,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["ROOT_DIR = '/content/gdrive/MyDrive/Colab Notebooks'\n","DATA_DIR = os.path.join(ROOT_DIR, 'data', 'disaster_tweets')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZTnmHY9MeO-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616163474616,"user_tz":-60,"elapsed":2294,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"aebdac89-42c3-417a-f6ee-273a6a248ebd"},"source":["print(ROOT_DIR)\n","print(DATA_DIR)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/Colab Notebooks\n","/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2iX6V9JpM9Am","executionInfo":{"status":"ok","timestamp":1616163474617,"user_tz":-60,"elapsed":2294,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["if ROOT_DIR not in sys.path:\n","  sys.path.append(ROOT_DIR)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"JT2FKCSaNim3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616163477309,"user_tz":-60,"elapsed":4984,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"812cfa8f-3f90-4c7d-9e65-9ce31caa17da"},"source":["!pip install -r \"$ROOT_DIR/requirementscl.txt\""],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 1)) (0.1.0)\n","Requirement already satisfied: datasets>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (1.5.0)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 3)) (0.1.95)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (4.4.2)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 1)) (1.8.0+cu101)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (0.3.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (1.19.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (0.8.7)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (4.41.1)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (0.0.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (1.1.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (0.70.11.1)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (3.7.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2.0.0)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (3.0.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (20.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (0.10.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2.8.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (3.4.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2020.12.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i-QY3B4lNfnT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164423685,"user_tz":-60,"elapsed":546,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"86d7b73d-b1c9-40b1-ee06-ca9bad594305"},"source":["##############\n","# PARAMETERS #\n","##############\n","model_name = f\"roberta-base\"\n","num_epochs = \"3\"\n","timestamp=str(datetime.now()).replace(' ','_').replace(':','').replace('-','').split('.')[0][2:-2]\n","\n","model_results_name = f\"{model_name}_{num_epochs}e_{timestamp}\"\n","\n","args = ['run_tf_text_classification.py']\n","args.extend([\n","  \"--train_file\", os.path.join(DATA_DIR, \"train_formatted.csv\"),\n","  \"--test_file\", os.path.join(DATA_DIR, \"test_formatted.csv\"),\n","  \"--dev_file\", os.path.join(DATA_DIR, \"valid_formatted.csv\"),\n","  \"--label_column_id\", \"0\",\n","  \"--model_name_or_path\", model_name, \n","  \"--output_dir\", os.path.join(DATA_DIR, f\"mod_{model_name}\"), \n","  \"--num_train_epochs\", num_epochs,\n","  \"--per_device_train_batch_size\", \"16\",\n","  \"--per_device_eval_batch_size\", \"32\",\n","  \"--do_train\", \n","  \"--do_eval\",\n","  \"--do_predict\", \n","  \"--logging_steps\", \"460\", ### 460 for test 256, 444 for test 512, 476 for all records in training\n","  \"--evaluation_strategy\", \"steps\", \n","  \"--save_steps\", \"460\", ### WAS: \"476\", ### 2 times per epoch (476 steps per epoch)\n","  \"--overwrite_output_dir\", \n","  \"--max_seq_length\", \"128\"    \n","])\n","\n","print(f\"MODEL NAME: {model_name}\")\n","print(f\"RESULTS NAME: {model_results_name}\")"],"execution_count":28,"outputs":[{"output_type":"stream","text":["MODEL NAME: roberta-base\n","RESULTS NAME: roberta-base_3e_210319_1433\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Cln06k3OVeyw","executionInfo":{"status":"ok","timestamp":1616164426068,"user_tz":-60,"elapsed":536,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["#!/usr/bin/env python\n","# coding=utf-8\n","# Copyright 2020 The HuggingFace Team. All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\" Fine-tuning the library models for sequence classification.\"\"\"\n","import logging\n","import os\n","from dataclasses import dataclass, field\n","from typing import Dict, Optional\n","\n","import datasets\n","import numpy as np\n","import tensorflow as tf\n","\n","from transformers import (\n","    AutoConfig,\n","    AutoTokenizer,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    PreTrainedTokenizer,\n","    TFAutoModelForSequenceClassification,\n","    TFTrainer,\n","    TFTrainingArguments,\n",")\n","from transformers.utils import logging as hf_logging\n","\n","\n","hf_logging.set_verbosity_info()\n","hf_logging.enable_default_handler()\n","hf_logging.enable_explicit_format()\n","\n","\n","def get_tfds(\n","    train_file: str,\n","    eval_file: str,\n","    test_file: str,\n","    tokenizer: PreTrainedTokenizer,\n","    label_column_id: int,\n","    max_seq_length: Optional[int] = None,\n","):\n","    files = {}\n","\n","    if train_file is not None:\n","        files[datasets.Split.TRAIN] = [train_file]\n","    if eval_file is not None:\n","        files[datasets.Split.VALIDATION] = [eval_file]\n","    if test_file is not None:\n","        files[datasets.Split.TEST] = [test_file]\n","\n","    ds = datasets.load_dataset(\"csv\", data_files=files)\n","    features_name = list(ds[list(files.keys())[0]].features.keys())\n","    print(f\"Features Name: {features_name}\")\n","    \n","    label_name = features_name.pop(label_column_id)\n","    label_list = list(set(ds[list(files.keys())[0]][label_name]))\n","    label2id = {label: i for i, label in enumerate(label_list)}\n","    input_names = tokenizer.model_input_names\n","    transformed_ds = {}\n","\n","    if len(features_name) == 1:\n","        for k in files.keys():\n","            transformed_ds[k] = ds[k].map(\n","                lambda example: tokenizer.batch_encode_plus(\n","                    example[features_name[0]], truncation=True, max_length=max_seq_length, padding=\"max_length\"\n","                ),\n","                batched=True,\n","            )\n","    elif len(features_name) == 2:\n","        for k in files.keys():\n","            transformed_ds[k] = ds[k].map(\n","                lambda example: tokenizer.batch_encode_plus(\n","                    (example[features_name[0]], example[features_name[1]]),\n","                    truncation=True,\n","                    max_length=max_seq_length,\n","                    padding=\"max_length\",\n","                ),\n","                batched=True,\n","            )\n","\n","    def gen_train():\n","        for ex in transformed_ds[datasets.Split.TRAIN]:\n","            d = {k: v for k, v in ex.items() if k in input_names}\n","            label = label2id[ex[label_name]]\n","            yield (d, label)\n","\n","    def gen_val():\n","        for ex in transformed_ds[datasets.Split.VALIDATION]:\n","            d = {k: v for k, v in ex.items() if k in input_names}\n","            label = label2id[ex[label_name]]\n","            yield (d, label)\n","\n","    def gen_test():\n","        for ex in transformed_ds[datasets.Split.TEST]:\n","            d = {k: v for k, v in ex.items() if k in input_names}\n","            label = label2id[ex[label_name]]\n","            yield (d, label)\n","\n","    train_ds = (\n","        tf.data.Dataset.from_generator(\n","            gen_train,\n","            ({k: tf.int32 for k in input_names}, tf.int64),\n","            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n","        )\n","        if datasets.Split.TRAIN in transformed_ds\n","        else None\n","    )\n","\n","    if train_ds is not None:\n","        train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(len(ds[datasets.Split.TRAIN])))\n","\n","    val_ds = (\n","        tf.data.Dataset.from_generator(\n","            gen_val,\n","            ({k: tf.int32 for k in input_names}, tf.int64),\n","            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n","        )\n","        if datasets.Split.VALIDATION in transformed_ds\n","        else None\n","    )\n","\n","    if val_ds is not None:\n","        val_ds = val_ds.apply(tf.data.experimental.assert_cardinality(len(ds[datasets.Split.VALIDATION])))\n","\n","    test_ds = (\n","        tf.data.Dataset.from_generator(\n","            gen_test,\n","            ({k: tf.int32 for k in input_names}, tf.int64),\n","            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n","        )\n","        if datasets.Split.TEST in transformed_ds\n","        else None\n","    )\n","\n","    if test_ds is not None:\n","        test_ds = test_ds.apply(tf.data.experimental.assert_cardinality(len(ds[datasets.Split.TEST])))\n","\n","    return train_ds, val_ds, test_ds, label2id\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    label_column_id: int = field(metadata={\"help\": \"Which column contains the label\"})\n","    train_file: str = field(default=None, metadata={\"help\": \"The path of the training file\"})\n","    dev_file: Optional[str] = field(default=None, metadata={\"help\": \"The path of the development file\"})\n","    test_file: Optional[str] = field(default=None, metadata={\"help\": \"The path of the test file\"})\n","    max_seq_length: int = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n","    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n","    # or just modify its tokenizer_config.json.\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Qs-q_w9V3mi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164426785,"user_tz":-60,"elapsed":374,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"3c20938c-1a78-4dcb-91f2-662ad9c55445"},"source":["sys.argv = args\n","\n","# See all possible arguments in src/transformers/training_args.py\n","# or by passing the --help flag to this script.\n","# We now keep distinct sets of args, for a cleaner separation of concerns.\n","parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n","model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n","\n","if (\n","    os.path.exists(training_args.output_dir)\n","    and os.listdir(training_args.output_dir)\n","    and training_args.do_train\n","    and not training_args.overwrite_output_dir\n","):\n","    raise ValueError(\n","        f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n","    )\n","\n","# Setup logging\n","logging.basicConfig(\n","    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","    datefmt=\"%m/%d/%Y %H:%M:%S\",\n","    level=logging.INFO,\n",")\n","logger.info(\n","    \"n_replicas: %s, distributed training: %s, 16-bits training: %s\",\n","    training_args.n_replicas,\n","    bool(training_args.n_replicas > 1),\n","    training_args.fp16,\n",")\n","logger.info(\"Training/evaluation parameters %s\", training_args)\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["[INFO|training_args.py:631] 2021-03-19 14:33:46,658 >> PyTorch: setting up devices\n","[INFO|training_args.py:555] 2021-03-19 14:33:46,660 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","[INFO|training_args_tf.py:192] 2021-03-19 14:33:46,664 >> Tensorflow: setting up strategy\n","03/19/2021 14:33:46 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n","03/19/2021 14:33:46 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Mar19_14-33-46_a9dc039fb8be', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=460, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=460, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=460, dataloader_num_workers=0, past_index=-1, run_name='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, tpu_name=None, tpu_zone=None, gcp_project=None, poly_power=1.0, xla=False)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"EvshoZjemqBT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164427889,"user_tz":-60,"elapsed":978,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"6e4ebf26-c9db-468f-ae1c-b8348d251070"},"source":["# Load pretrained model and tokenizer\n","#\n","# Distributed training:\n","# The .from_pretrained methods guarantee that only one local process can concurrently\n","# download model & vocab.\n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","    cache_dir=model_args.cache_dir,\n",")\n"],"execution_count":31,"outputs":[{"output_type":"stream","text":["[INFO|configuration_utils.py:463] 2021-03-19 14:33:47,369 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:499] 2021-03-19 14:33:47,370 >> Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.4.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 14:33:47,491 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 14:33:47,491 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 14:33:47,495 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 14:33:47,496 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 14:33:47,500 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 14:33:47,501 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2IN_1e_pNemZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164427890,"user_tz":-60,"elapsed":596,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"cd25b1dc-9337-463f-ea4d-650a1d976ac9"},"source":["\n","train_dataset, eval_dataset, test_ds, label2id = get_tfds(\n","    train_file=data_args.train_file,\n","    eval_file=data_args.dev_file,\n","    test_file=data_args.test_file,\n","    tokenizer=tokenizer,\n","    label_column_id=data_args.label_column_id,\n","    max_seq_length=data_args.max_seq_length,\n",")\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["03/19/2021 14:33:47 - WARNING - datasets.builder -   Using custom data configuration default-6a3641a1c4d40622\n","03/19/2021 14:33:47 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-6a3641a1c4d40622/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n","03/19/2021 14:33:47 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-6a3641a1c4d40622/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-98df6cf059ccd6ab.arrow\n","03/19/2021 14:33:47 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-6a3641a1c4d40622/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-07b371ff0467f778.arrow\n","03/19/2021 14:33:47 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-6a3641a1c4d40622/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-da2e19f75a19afe0.arrow\n"],"name":"stderr"},{"output_type":"stream","text":["Features Name: ['target', 'text']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"owHluSoZVCpz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164430556,"user_tz":-60,"elapsed":591,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"4b064f11-e55b-4578-a67d-f97d78e6afbc"},"source":["config = AutoConfig.from_pretrained(\n","    model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","    num_labels=len(label2id),\n","    label2id=label2id,\n","    id2label={id: label for label, id in label2id.items()},\n","    finetuning_task=\"text-classification\",\n","    cache_dir=model_args.cache_dir,\n",")\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["[INFO|configuration_utils.py:463] 2021-03-19 14:33:50,230 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:499] 2021-03-19 14:33:50,231 >> Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"text-classification\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.4.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eq0_GvnNWWVC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164445504,"user_tz":-60,"elapsed":15088,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"d2c12d2c-dbf6-4e06-bff0-c5aafd79aacd"},"source":["with training_args.strategy.scope():\n","    model = TFAutoModelForSequenceClassification.from_pretrained(\n","        model_args.model_name_or_path,\n","        from_pt=bool(\".bin\" in model_args.model_name_or_path),\n","        config=config,\n","        cache_dir=model_args.cache_dir,\n","    )\n","\n"],"execution_count":34,"outputs":[{"output_type":"stream","text":["[INFO|modeling_tf_utils.py:1240] 2021-03-19 14:33:50,662 >> loading weights file https://huggingface.co/roberta-base/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/22fef2e3c5012c1a8f8d7f024e30275dd2925b076abb5131dc3d1068345b6426.d409db346b0c1408865b9785d36744ccb988186626309ae8f995f86511811602.h5\n","[WARNING|modeling_tf_utils.py:1298] 2021-03-19 14:34:05,139 >> All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","[WARNING|modeling_tf_utils.py:1302] 2021-03-19 14:34:05,140 >> Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"z7ibI-x8WZ6R","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164445722,"user_tz":-60,"elapsed":15098,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"f40af8fa-cba1-4bb0-b8f5-cd0ab33bbaec"},"source":["def compute_metrics(p: EvalPrediction) -> Dict:\n","    preds = np.argmax(p.predictions, axis=1)\n","\n","    return {\"acc\": (preds == p.label_ids).mean()}\n","\n","# Initialize our Trainer\n","trainer = TFTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    compute_metrics=compute_metrics,\n",")\n","\n"],"execution_count":35,"outputs":[{"output_type":"stream","text":["[INFO|trainer_tf.py:117] 2021-03-19 14:34:05,168 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n","[INFO|trainer_tf.py:125] 2021-03-19 14:34:05,170 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"TRJPLNpqrKtJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164445723,"user_tz":-60,"elapsed":14932,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"9573076b-4a1f-48bd-e936-9f4ccdc9a86a"},"source":["print(f\"train: {len(train_dataset)}\")\n","print(f\"valid: {len(eval_dataset)}\")\n","print(f\"test: {len(test_ds)}\")\n","print(f\"Model name: {model_name}\")\n","print(f\"Results name: {model_results_name}\")"],"execution_count":36,"outputs":[{"output_type":"stream","text":["train: 7357\n","valid: 256\n","test: 3263\n","Model name: roberta-base\n","Results name: roberta-base_3e_210319_1433\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qTnovXqgWjG-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164684183,"user_tz":-60,"elapsed":253218,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"c58bcd51-efac-4b61-e106-539d9ab1a14b"},"source":["# Training\n","if training_args.do_train:\n","    trainer.train()\n","    trainer.save_model()\n","    tokenizer.save_pretrained(training_args.output_dir)\n"],"execution_count":37,"outputs":[{"output_type":"stream","text":["[INFO|trainer_tf.py:507] 2021-03-19 14:34:05,498 >> Checkpoint file /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/checkpoint/ckpt-2 found and restoring from checkpoint\n","[INFO|trainer_tf.py:516] 2021-03-19 14:34:06,553 >>   Continuing training from checkpoint, will skip to saved global_step\n","[INFO|trainer_tf.py:517] 2021-03-19 14:34:06,554 >>   Continuing training from epoch 2\n","[INFO|trainer_tf.py:518] 2021-03-19 14:34:06,556 >>   Continuing training from global step 920\n","[INFO|trainer_tf.py:519] 2021-03-19 14:34:06,557 >>   Will skip the first 0 steps in the first epoch\n","[INFO|trainer_tf.py:528] 2021-03-19 14:34:06,561 >> ***** Running training *****\n","[INFO|trainer_tf.py:529] 2021-03-19 14:34:06,562 >>   Num examples = 7357\n","[INFO|trainer_tf.py:531] 2021-03-19 14:34:06,563 >>   Num Epochs = 3\n","[INFO|trainer_tf.py:532] 2021-03-19 14:34:06,564 >>   Instantaneous batch size per device = 16\n","[INFO|trainer_tf.py:534] 2021-03-19 14:34:06,566 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer_tf.py:536] 2021-03-19 14:34:06,566 >>   Gradient Accumulation steps = 1\n","[INFO|trainer_tf.py:537] 2021-03-19 14:34:06,568 >>   Steps per epoch = 460\n","[INFO|trainer_tf.py:538] 2021-03-19 14:34:06,569 >>   Total optimization steps = 1380\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:34:09 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:34:09 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:34:17 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:34:17 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:306] 2021-03-19 14:37:27,846 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 14:37:27,848 >>   Num examples in dataset = 256\n","[INFO|trainer_tf.py:309] 2021-03-19 14:37:27,849 >>   Num examples in used in evaluation = 256\n","[INFO|trainer_tf.py:310] 2021-03-19 14:37:27,850 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:37:27 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:37:27 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 14:37:31,343 >> {'eval_loss': 0.6194115281105042, 'eval_acc': 0.796875, 'epoch': 3.0, 'step': 1380}\n","[INFO|trainer_tf.py:404] 2021-03-19 14:37:31,352 >> {'loss': 0.2004595, 'learning_rate': 0.0, 'epoch': 3.0, 'step': 1380}\n","[INFO|trainer_tf.py:595] 2021-03-19 14:37:38,812 >> Saving checkpoint for step 1380 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/checkpoint/ckpt-3\n","[INFO|trainer_tf.py:610] 2021-03-19 14:37:38,833 >> Training took: 0:03:32.255750\n","[INFO|trainer_tf.py:785] 2021-03-19 14:37:38,834 >> Saving model in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base\n","[INFO|configuration_utils.py:314] 2021-03-19 14:37:38,845 >> Configuration saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/config.json\n","[INFO|modeling_tf_utils.py:1045] 2021-03-19 14:38:03,765 >> Model weights saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/tf_model.h5\n","[INFO|tokenization_utils_base.py:1896] 2021-03-19 14:38:03,783 >> tokenizer config file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-03-19 14:38:03,792 >> Special tokens file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/special_tokens_map.json\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"67aDVc14WrPG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164688588,"user_tz":-60,"elapsed":257479,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"f94a754e-648a-4982-a45e-6e6b93aa8cbc"},"source":["# Evaluation\n","results = {}\n","if training_args.do_eval:\n","    logger.info(\"*** Evaluate ***\")\n","    result = trainer.evaluate()\n","    output_eval_file = os.path.join(training_args.output_dir, f\"eval_results_{model_results_name}.txt\")\n","\n","    with open(output_eval_file, \"w\") as writer:\n","        logger.info(\"***** Eval results *****\")\n","\n","        for key, value in result.items():\n","            logger.info(\"  %s = %s\", key, value)\n","            writer.write(\"%s = %s\\n\" % (key, value))\n","\n","        results.update(result)\n"],"execution_count":38,"outputs":[{"output_type":"stream","text":["03/19/2021 14:38:03 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer_tf.py:306] 2021-03-19 14:38:03,964 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 14:38:03,964 >>   Num examples in dataset = 256\n","[INFO|trainer_tf.py:309] 2021-03-19 14:38:03,965 >>   Num examples in used in evaluation = 256\n","[INFO|trainer_tf.py:310] 2021-03-19 14:38:03,969 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:38:04 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:38:04 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 14:38:07,856 >> {'eval_loss': 0.6194115281105042, 'eval_acc': 0.796875, 'epoch': 3.0, 'step': 1380}\n","03/19/2021 14:38:07 - INFO - __main__ -   ***** Eval results *****\n","03/19/2021 14:38:07 - INFO - __main__ -     eval_loss = 0.6194115281105042\n","03/19/2021 14:38:07 - INFO - __main__ -     eval_acc = 0.796875\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"tcx4G3qhbNLh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164688589,"user_tz":-60,"elapsed":257333,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"cce31c08-310a-4419-d804-b312cc025b01"},"source":["results"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'eval_acc': 0.796875, 'eval_loss': 0.6194115281105042}"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"A8TIRcW5dG09","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164688590,"user_tz":-60,"elapsed":257175,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"bfbcbe12-9c7c-40c0-901c-7642d150b257"},"source":["print(label2id)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["{0: 0, 1: 1}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e5A4agyobZW2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164714928,"user_tz":-60,"elapsed":282711,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"c653d822-cc85-40e4-ec9d-d20ea74dfa26"},"source":["# Prediction\n","if training_args.do_predict:\n","    logger.info(\"*** predictions ***\")\n","    preds = trainer.predict(test_ds)\n","\n","    logger.info(\"*** RESUTS: ***\")\n","    logger.info(preds)\n","    logger.info(\"*** :RESUTS ***\")\n"],"execution_count":41,"outputs":[{"output_type":"stream","text":["03/19/2021 14:38:07 - INFO - __main__ -   *** predictions ***\n","[INFO|trainer_tf.py:306] 2021-03-19 14:38:07,906 >> ***** Running Prediction *****\n","[INFO|trainer_tf.py:307] 2021-03-19 14:38:07,907 >>   Num examples in dataset = 3263\n","[INFO|trainer_tf.py:310] 2021-03-19 14:38:07,924 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:38:32 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 14:38:32 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","03/19/2021 14:38:34 - INFO - __main__ -   *** RESUTS: ***\n","03/19/2021 14:38:34 - INFO - __main__ -   PredictionOutput(predictions=array([[-2.328225 ,  2.0874493],\n","       [-2.172442 ,  1.8221797],\n","       [-2.8755362,  2.4670014],\n","       ...,\n","       [-3.0634918,  2.8123376],\n","       [-1.0641627,  1.3007073],\n","       [-1.9410375,  1.8270208]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 0]), metrics={'eval_loss': 1.8061743343577665, 'eval_acc': 0.5822862396567576})\n","03/19/2021 14:38:34 - INFO - __main__ -   *** :RESUTS ***\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"iQ_cMlWEjgOp","executionInfo":{"status":"ok","timestamp":1616164714929,"user_tz":-60,"elapsed":282306,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["decoded_preds = np.argmax(preds.predictions, axis=1)"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"_7VBLY36lpuY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616164714929,"user_tz":-60,"elapsed":281789,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"f5131b63-d516-4fe9-d718-4eb341aebc80"},"source":["decoded_preds[10:30]"],"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1])"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"id":"XfmRpIlRlyqN","executionInfo":{"status":"ok","timestamp":1616164714930,"user_tz":-60,"elapsed":281098,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["org_test = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jslcl5dVmDuu","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1616164714930,"user_tz":-60,"elapsed":280553,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"a1027fbe-828e-4c89-d004-4524917d881a"},"source":["org_test.head()"],"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just happened a terrible car crash</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Heard about #earthquake is different cities, s...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>there is a forest fire at spot pond, geese are...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Apocalypse lighting. #Spokane #wildfires</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword location                                               text\n","0   0     NaN      NaN                 Just happened a terrible car crash\n","1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n","2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n","3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n","4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"code","metadata":{"id":"SRsruQqAmJfU","executionInfo":{"status":"ok","timestamp":1616164714931,"user_tz":-60,"elapsed":279903,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["org_test['target'] = decoded_preds"],"execution_count":46,"outputs":[]},{"cell_type":"code","metadata":{"id":"iMjq7mESmNq3","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1616164714932,"user_tz":-60,"elapsed":279346,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"7fd8555f-a49f-4d56-8739-2eb050d27099"},"source":["org_test.head()"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just happened a terrible car crash</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Heard about #earthquake is different cities, s...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>there is a forest fire at spot pond, geese are...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Apocalypse lighting. #Spokane #wildfires</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword  ...                                               text target\n","0   0     NaN  ...                 Just happened a terrible car crash      1\n","1   2     NaN  ...  Heard about #earthquake is different cities, s...      1\n","2   3     NaN  ...  there is a forest fire at spot pond, geese are...      1\n","3   9     NaN  ...           Apocalypse lighting. #Spokane #wildfires      1\n","4  11     NaN  ...      Typhoon Soudelor kills 28 in China and Taiwan      0\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"nL-zxwmlmOpa","executionInfo":{"status":"ok","timestamp":1616164714932,"user_tz":-60,"elapsed":278810,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["org_test[['id', 'target']].to_csv(os.path.join(DATA_DIR, f'./sub_{model_results_name}.csv'), index=False)"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"dFM74dcKnACk","executionInfo":{"status":"ok","timestamp":1616163996761,"user_tz":-60,"elapsed":524401,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":[""],"execution_count":27,"outputs":[]}]}