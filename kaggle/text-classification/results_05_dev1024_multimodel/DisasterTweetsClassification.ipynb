{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DisasterTweetsClassification.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPSz9otv/NmI/PWGGFhiTAB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"w3y3qLlcMiVw","executionInfo":{"status":"ok","timestamp":1616193252522,"user_tz":-60,"elapsed":568,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["import os\n","import sys\n","import pandas as pd\n","from datetime import datetime\n","\n","from google.colab import drive"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVv_i1iHMjOi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616193254016,"user_tz":-60,"elapsed":2055,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"8d027d1e-9492-42c0-abf3-fd1bef569c80"},"source":["drive.mount('/content/gdrive', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PtmlVcbwMuqH","executionInfo":{"status":"ok","timestamp":1616193254017,"user_tz":-60,"elapsed":2052,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["ROOT_DIR = '/content/gdrive/MyDrive/Colab Notebooks'\n","DATA_DIR = os.path.join(ROOT_DIR, 'data', 'disaster_tweets')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"pZTnmHY9MeO-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616193254017,"user_tz":-60,"elapsed":2047,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"d9ef0bae-ea96-4c9a-a4a7-533b04f98374"},"source":["print(ROOT_DIR)\n","print(DATA_DIR)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/Colab Notebooks\n","/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2iX6V9JpM9Am","executionInfo":{"status":"ok","timestamp":1616193254018,"user_tz":-60,"elapsed":2044,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["if ROOT_DIR not in sys.path:\n","  sys.path.append(ROOT_DIR)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"JT2FKCSaNim3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616193256770,"user_tz":-60,"elapsed":4792,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"c07d2275-57b6-4556-a860-9cdefa360f91"},"source":["!pip install -r \"$ROOT_DIR/requirementscl.txt\""],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 1)) (0.1.0)\n","Requirement already satisfied: datasets>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (1.5.0)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 3)) (0.1.95)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (4.4.2)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 1)) (1.8.0+cu101)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (1.19.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (0.70.11.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (0.8.7)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (0.0.7)\n","Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (4.41.1)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (3.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (1.1.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (3.7.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (0.3.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2.0.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (20.9)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (0.10.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (3.0.12)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2.8.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (3.4.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets>=1.1.3->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 2)) (2020.12.5)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->-r /content/gdrive/MyDrive/Colab Notebooks/requirementscl.txt (line 4)) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Cln06k3OVeyw","executionInfo":{"status":"ok","timestamp":1616193262409,"user_tz":-60,"elapsed":10425,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["#!/usr/bin/env python\n","# coding=utf-8\n","# Copyright 2020 The HuggingFace Team. All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\" Fine-tuning the library models for sequence classification.\"\"\"\n","import logging\n","import os\n","from dataclasses import dataclass, field\n","from typing import Dict, Optional\n","\n","import datasets\n","import numpy as np\n","import tensorflow as tf\n","\n","from transformers import (\n","    AutoConfig,\n","    AutoTokenizer,\n","    EvalPrediction,\n","    HfArgumentParser,\n","    PreTrainedTokenizer,\n","    TFAutoModelForSequenceClassification,\n","    TFTrainer,\n","    TFTrainingArguments,\n",")\n","from transformers.utils import logging as hf_logging\n","\n","\n","hf_logging.set_verbosity_info()\n","hf_logging.enable_default_handler()\n","hf_logging.enable_explicit_format()\n","\n","\n","def get_tfds(\n","    train_file: str,\n","    eval_file: str,\n","    test_file: str,\n","    tokenizer: PreTrainedTokenizer,\n","    label_column_id: int,\n","    max_seq_length: Optional[int] = None,\n","):\n","    files = {}\n","\n","    if train_file is not None:\n","        files[datasets.Split.TRAIN] = [train_file]\n","    if eval_file is not None:\n","        files[datasets.Split.VALIDATION] = [eval_file]\n","    if test_file is not None:\n","        files[datasets.Split.TEST] = [test_file]\n","\n","    ds = datasets.load_dataset(\"csv\", data_files=files)\n","    features_name = list(ds[list(files.keys())[0]].features.keys())\n","    print(f\"Features Name: {features_name}\")\n","    \n","    label_name = features_name.pop(label_column_id)\n","    label_list = list(set(ds[list(files.keys())[0]][label_name]))\n","    label2id = {label: i for i, label in enumerate(label_list)}\n","    input_names = tokenizer.model_input_names\n","    transformed_ds = {}\n","\n","    if len(features_name) == 1:\n","        for k in files.keys():\n","            transformed_ds[k] = ds[k].map(\n","                lambda example: tokenizer.batch_encode_plus(\n","                    example[features_name[0]], truncation=True, max_length=max_seq_length, padding=\"max_length\"\n","                ),\n","                batched=True,\n","            )\n","    elif len(features_name) == 2:\n","        for k in files.keys():\n","            transformed_ds[k] = ds[k].map(\n","                lambda example: tokenizer.batch_encode_plus(\n","                    (example[features_name[0]], example[features_name[1]]),\n","                    truncation=True,\n","                    max_length=max_seq_length,\n","                    padding=\"max_length\",\n","                ),\n","                batched=True,\n","            )\n","\n","    def gen_train():\n","        for ex in transformed_ds[datasets.Split.TRAIN]:\n","            d = {k: v for k, v in ex.items() if k in input_names}\n","            label = label2id[ex[label_name]]\n","            yield (d, label)\n","\n","    def gen_val():\n","        for ex in transformed_ds[datasets.Split.VALIDATION]:\n","            d = {k: v for k, v in ex.items() if k in input_names}\n","            label = label2id[ex[label_name]]\n","            yield (d, label)\n","\n","    def gen_test():\n","        for ex in transformed_ds[datasets.Split.TEST]:\n","            d = {k: v for k, v in ex.items() if k in input_names}\n","            label = label2id[ex[label_name]]\n","            yield (d, label)\n","\n","    train_ds = (\n","        tf.data.Dataset.from_generator(\n","            gen_train,\n","            ({k: tf.int32 for k in input_names}, tf.int64),\n","            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n","        )\n","        if datasets.Split.TRAIN in transformed_ds\n","        else None\n","    )\n","\n","    if train_ds is not None:\n","        train_ds = train_ds.apply(tf.data.experimental.assert_cardinality(len(ds[datasets.Split.TRAIN])))\n","\n","    val_ds = (\n","        tf.data.Dataset.from_generator(\n","            gen_val,\n","            ({k: tf.int32 for k in input_names}, tf.int64),\n","            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n","        )\n","        if datasets.Split.VALIDATION in transformed_ds\n","        else None\n","    )\n","\n","    if val_ds is not None:\n","        val_ds = val_ds.apply(tf.data.experimental.assert_cardinality(len(ds[datasets.Split.VALIDATION])))\n","\n","    test_ds = (\n","        tf.data.Dataset.from_generator(\n","            gen_test,\n","            ({k: tf.int32 for k in input_names}, tf.int64),\n","            ({k: tf.TensorShape([None]) for k in input_names}, tf.TensorShape([])),\n","        )\n","        if datasets.Split.TEST in transformed_ds\n","        else None\n","    )\n","\n","    if test_ds is not None:\n","        test_ds = test_ds.apply(tf.data.experimental.assert_cardinality(len(ds[datasets.Split.TEST])))\n","\n","    return train_ds, val_ds, test_ds, label2id\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","@dataclass\n","class DataTrainingArguments:\n","    \"\"\"\n","    Arguments pertaining to what data we are going to input our model for training and eval.\n","\n","    Using `HfArgumentParser` we can turn this class\n","    into argparse arguments to be able to specify them on\n","    the command line.\n","    \"\"\"\n","\n","    label_column_id: int = field(metadata={\"help\": \"Which column contains the label\"})\n","    train_file: str = field(default=None, metadata={\"help\": \"The path of the training file\"})\n","    dev_file: Optional[str] = field(default=None, metadata={\"help\": \"The path of the development file\"})\n","    test_file: Optional[str] = field(default=None, metadata={\"help\": \"The path of the test file\"})\n","    max_seq_length: int = field(\n","        default=128,\n","        metadata={\n","            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n","            \"than this will be truncated, sequences shorter will be padded.\"\n","        },\n","    )\n","    overwrite_cache: bool = field(\n","        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n","    )\n","\n","\n","@dataclass\n","class ModelArguments:\n","    \"\"\"\n","    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n","    \"\"\"\n","\n","    model_name_or_path: str = field(\n","        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n","    )\n","    config_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n","    )\n","    tokenizer_name: Optional[str] = field(\n","        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n","    )\n","    use_fast: bool = field(default=False, metadata={\"help\": \"Set this flag to use fast tokenization.\"})\n","    # If you want to tweak more attributes on your tokenizer, you should do it in a distinct script,\n","    # or just modify its tokenizer_config.json.\n","    cache_dir: Optional[str] = field(\n","        default=None,\n","        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n","    )"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Qs-q_w9V3mi","executionInfo":{"status":"ok","timestamp":1616193262410,"user_tz":-60,"elapsed":10421,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":["def run_training(model_name, results_name):\n","  # See all possible arguments in src/transformers/training_args.py\n","  # or by passing the --help flag to this script.\n","  # We now keep distinct sets of args, for a cleaner separation of concerns.\n","  parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n","  model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n","\n","  if (\n","      os.path.exists(training_args.output_dir)\n","      and os.listdir(training_args.output_dir)\n","      and training_args.do_train\n","      and not training_args.overwrite_output_dir\n","  ):\n","      raise ValueError(\n","          f\"Output directory ({training_args.output_dir}) already exists and is not empty. Use --overwrite_output_dir to overcome.\"\n","      )\n","\n","  # Setup logging\n","  logging.basicConfig(\n","      format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","      datefmt=\"%m/%d/%Y %H:%M:%S\",\n","      level=logging.INFO,\n","  )\n","  logger.info(\n","      \"n_replicas: %s, distributed training: %s, 16-bits training: %s\",\n","      training_args.n_replicas,\n","      bool(training_args.n_replicas > 1),\n","      training_args.fp16,\n","  )\n","  logger.info(\"Training/evaluation parameters %s\", training_args)\n","\n","  # Load pretrained model and tokenizer\n","  #\n","  # Distributed training:\n","  # The .from_pretrained methods guarantee that only one local process can concurrently\n","  # download model & vocab.\n","\n","  tokenizer = AutoTokenizer.from_pretrained(\n","      model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n","      cache_dir=model_args.cache_dir,\n","  )\n","\n","  train_dataset, eval_dataset, test_ds, label2id = get_tfds(\n","      train_file=data_args.train_file,\n","      eval_file=data_args.dev_file,\n","      test_file=data_args.test_file,\n","      tokenizer=tokenizer,\n","      label_column_id=data_args.label_column_id,\n","      max_seq_length=data_args.max_seq_length,\n","  )\n","\n","  config = AutoConfig.from_pretrained(\n","      model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n","      num_labels=len(label2id),\n","      label2id=label2id,\n","      id2label={id: label for label, id in label2id.items()},\n","      finetuning_task=\"text-classification\",\n","      cache_dir=model_args.cache_dir,\n","  )\n","\n","  with training_args.strategy.scope():\n","      model = TFAutoModelForSequenceClassification.from_pretrained(\n","          model_args.model_name_or_path,\n","          from_pt=bool(\".bin\" in model_args.model_name_or_path),\n","          config=config,\n","          cache_dir=model_args.cache_dir,\n","      )\n","\n","  def compute_metrics(p: EvalPrediction) -> Dict:\n","      preds = np.argmax(p.predictions, axis=1)\n","\n","      return {\"acc\": (preds == p.label_ids).mean()}\n","\n","  # Initialize our Trainer\n","  trainer = TFTrainer(\n","      model=model,\n","      args=training_args,\n","      train_dataset=train_dataset,\n","      eval_dataset=eval_dataset,\n","      compute_metrics=compute_metrics,\n","  )\n","\n","  print(f\"train: {len(train_dataset)}\")\n","  print(f\"valid: {len(eval_dataset)}\")\n","  print(f\"test: {len(test_ds)}\")\n","  print(f\"Model name: {model_name}\")\n","  print(f\"Results name: {model_results_name}\")\n","\n","  # Training\n","  if training_args.do_train:\n","      trainer.train()\n","      trainer.save_model()\n","      tokenizer.save_pretrained(training_args.output_dir)\n","\n","  # Evaluation\n","  results = {}\n","  if training_args.do_eval:\n","      logger.info(\"*** Evaluate ***\")\n","      result = trainer.evaluate()\n","      output_eval_file = os.path.join(training_args.output_dir, f\"eval_results_{model_results_name}.txt\")\n","\n","      with open(output_eval_file, \"w\") as writer:\n","          logger.info(\"***** Eval results *****\")\n","\n","          for key, value in result.items():\n","              logger.info(\"  %s = %s\", key, value)\n","              writer.write(\"%s = %s\\n\" % (key, value))\n","\n","          results.update(result)\n","\n","          accuracy = results[[k for k in results.keys() if \"acc\" in k][0]]\n","          accuracy_str = f\"a0{(int(round(accuracy*1000,0)))}\"\n","          print(accuracy_str)\n","  else:\n","    acuracy_str = 'aunk'\n","\n","  # Prediction\n","  if training_args.do_predict:\n","      logger.info(\"*** predictions ***\")\n","      preds = trainer.predict(test_ds)\n","\n","      logger.info(\"*** RESUTS: ***\")\n","      logger.info(preds)\n","      logger.info(\"*** :RESUTS ***\")\n","\n","      decoded_preds = np.argmax(preds.predictions, axis=1)\n","      org_test = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))\n","      org_test['target'] = decoded_preds\n","      org_test[['id', 'target']].to_csv(os.path.join(DATA_DIR, f'./sub_{accuracy_str}_{model_results_name}.csv'), index=False)\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"dFM74dcKnACk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616195910564,"user_tz":-60,"elapsed":2658571,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}},"outputId":"fe8717b9-df38-4d40-85c9-49594ee9ad31"},"source":["#############\n","# MAIN LOOP #\n","#############\n","import shutil\n","\n","# AVAILABLE MODELS:\n","# ConvBertConfig, DistilBertConfig, AlbertConfig, CamembertConfig, XLMRobertaConfig, LongformerConfig, \n","# RobertaConfig, BertConfig, XLNetConfig, MobileBertConfig, FlaubertConfig, XLMConfig, ElectraConfig, FunnelConfig, GPT2Config, MPNetConfig, \n","# OpenAIGPTConfig, TransfoXLConfig, CTRLConfig.\n","\n","models = ['distilbert-base-uncased', 'roberta-base', 'bert-base-uncased', 'bert-base-multilingual-uncased']\n","devsetmode = 'v1024o'\n","for model_name in models:\n","  num_epochs = \"3\"\n","  timestamp=str(datetime.now()).replace(' ','_').replace(':','').replace('-','').split('.')[0][2:-2]\n","\n","  model_results_name = f\"{model_name.replace('/', '-')}_{num_epochs}e_{devsetmode}_{timestamp}\"\n","  model_full_path = os.path.join(DATA_DIR, f\"mod_{model_name}\")\n","\n","  args = ['run_tf_text_classification.py']\n","  args.extend([\n","    \"--train_file\", os.path.join(DATA_DIR, \"train_formatted.csv\"),\n","    \"--test_file\", os.path.join(DATA_DIR, \"test_formatted.csv\"),\n","    \"--dev_file\", os.path.join(DATA_DIR, \"valid_formatted.csv\"),\n","    \"--label_column_id\", \"0\",\n","    \"--model_name_or_path\", model_name, \n","    \"--output_dir\", model_full_path, \n","    \"--num_train_epochs\", num_epochs,\n","    \"--per_device_train_batch_size\", \"16\",\n","    \"--per_device_eval_batch_size\", \"32\",\n","    \"--do_train\", \n","    \"--do_eval\",\n","    \"--do_predict\", \n","    \"--logging_steps\", \"476\", ### 460 for test 256, 444 for test 512, 476 for all records in training\n","    \"--evaluation_strategy\", \"steps\", \n","    \"--save_steps\", \"476\", ### WAS: \"476\", ### 2 times per epoch (476 steps per epoch)\n","    \"--overwrite_output_dir\", \n","    \"--max_seq_length\", \"128\"    \n","  ])\n","\n","  print(f\"*******************\")\n","  print(f\"*** Processing: ***\")\n","  print(f\"*******************\")\n","  print(f\"MODEL NAME: {model_name}\")\n","  print(f\"RESULTS NAME: {model_results_name}\")\n","  print(f\"TIMESTAMP: {timestamp}\")\n","  \n","  sys.argv = args\n","\n","  # Ensure all new files are accessible (it is not always the case for Google Drive)  \n","  drive.mount('/content/gdrive', force_remount=True)\n","\n","  run_training(model_name, model_results_name)\n","\n","  # Clean training artefacts\n","  logger.info(f\"removing checkpoints...\")\n","  shutil.rmtree(os.path.join(model_full_path, 'checkpoint'))\n","  \n","  files = os.listdir(model_full_path)\n","  for f in files:\n","    if os.path.splitext(f)[1] == '.h5':\n","      logger.info(f\"removing file {f}\")\n","      os.remove(os.path.join(model_full_path, f))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["*******************\n","*** Processing: ***\n","*******************\n","MODEL NAME: distilbert-base-uncased\n","RESULTS NAME: distilbert-base-uncased_3e_v1024o_210319_2234\n","TIMESTAMP: 210319_2234\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|training_args.py:631] 2021-03-19 22:34:23,164 >> PyTorch: setting up devices\n","[INFO|training_args.py:555] 2021-03-19 22:34:23,227 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","[INFO|training_args_tf.py:192] 2021-03-19 22:34:23,236 >> Tensorflow: setting up strategy\n"],"name":"stderr"},{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:34:23 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n","03/19/2021 22:34:23 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Mar19_22-34-23_a7d129682a59', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=476, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=476, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=476, dataloader_num_workers=0, past_index=-1, run_name='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, tpu_name=None, tpu_zone=None, gcp_project=None, poly_power=1.0, xla=False)\n","[INFO|configuration_utils.py:463] 2021-03-19 22:34:24,041 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n","[INFO|configuration_utils.py:499] 2021-03-19 22:34:24,042 >> Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.4.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:34:25,133 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:34:25,134 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:34:25,135 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:34:25,138 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:34:25,139 >> loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","03/19/2021 22:34:25 - WARNING - datasets.builder -   Using custom data configuration default-829a6b88a52ac715\n","03/19/2021 22:34:25 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n","03/19/2021 22:34:25 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-e915d6d93f239232.arrow\n","03/19/2021 22:34:25 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-3acd1955d926cb1f.arrow\n","03/19/2021 22:34:25 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-04ad7b94132852b8.arrow\n"],"name":"stderr"},{"output_type":"stream","text":["Features Name: ['target', 'text']\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|configuration_utils.py:463] 2021-03-19 22:34:25,774 >> loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361\n","[INFO|configuration_utils.py:499] 2021-03-19 22:34:25,775 >> Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"finetuning_task\": \"text-classification\",\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.4.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|modeling_tf_utils.py:1240] 2021-03-19 22:34:25,994 >> loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/fa107dc22c014df078a1b75235144a927f7e9764916222711f239b7ee6092ec9.bc4b731be56d8422e12b1d5bfa86fbd81d18d2770da1f5ac4f33640a17b7dde9.h5\n","[WARNING|modeling_tf_utils.py:1290] 2021-03-19 22:34:34,425 >> Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n","- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","[WARNING|modeling_tf_utils.py:1302] 2021-03-19 22:34:34,425 >> Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|trainer_tf.py:117] 2021-03-19 22:34:34,437 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n","[INFO|trainer_tf.py:125] 2021-03-19 22:34:34,438 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n","[INFO|trainer_tf.py:528] 2021-03-19 22:34:34,462 >> ***** Running training *****\n","[INFO|trainer_tf.py:529] 2021-03-19 22:34:34,463 >>   Num examples = 7613\n","[INFO|trainer_tf.py:531] 2021-03-19 22:34:34,465 >>   Num Epochs = 3\n","[INFO|trainer_tf.py:532] 2021-03-19 22:34:34,465 >>   Instantaneous batch size per device = 16\n","[INFO|trainer_tf.py:534] 2021-03-19 22:34:34,467 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer_tf.py:536] 2021-03-19 22:34:34,469 >>   Gradient Accumulation steps = 1\n","[INFO|trainer_tf.py:537] 2021-03-19 22:34:34,471 >>   Steps per epoch = 476\n","[INFO|trainer_tf.py:538] 2021-03-19 22:34:34,472 >>   Total optimization steps = 1428\n"],"name":"stderr"},{"output_type":"stream","text":["train: 7613\n","valid: 1024\n","test: 3263\n","Model name: distilbert-base-uncased\n","Results name: distilbert-base-uncased_3e_v1024o_210319_2234\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fa921eca050>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:34:40 - WARNING - tensorflow -   AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fa921eca050>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fa921eca050>> and will run it as-is.\n","Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n","Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7fa93d757c20> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:34:39 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n","03/19/2021 22:34:40 - WARNING - tensorflow -   AutoGraph could not transform <function wrap at 0x7fa93d757c20> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING: AutoGraph could not transform <function wrap at 0x7fa93d757c20> and will run it as-is.\n","Cause: while/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:35:00 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:35:24 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:35:24 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:306] 2021-03-19 22:37:03,672 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:37:03,673 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 22:37:03,674 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 22:37:03,675 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:37:03 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:37:03 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 22:37:09,202 >> {'eval_loss': 0.29513081908226013, 'eval_acc': 0.884765625, 'epoch': 1.0, 'step': 476}\n","[INFO|trainer_tf.py:404] 2021-03-19 22:37:09,213 >> {'loss': 0.41401082, 'learning_rate': 3.333333e-05, 'epoch': 1.0, 'step': 476}\n","[INFO|trainer_tf.py:595] 2021-03-19 22:37:12,667 >> Saving checkpoint for step 476 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased/checkpoint/ckpt-1\n","[INFO|trainer_tf.py:306] 2021-03-19 22:38:54,510 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:38:54,511 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 22:38:54,512 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 22:38:54,513 >>   Batch size = 32\n","[INFO|trainer_tf.py:404] 2021-03-19 22:38:59,113 >> {'eval_loss': 0.18254512548446655, 'eval_acc': 0.9375, 'epoch': 2.0, 'step': 952}\n","[INFO|trainer_tf.py:404] 2021-03-19 22:38:59,121 >> {'loss': 0.26426163, 'learning_rate': 1.6666665e-05, 'epoch': 2.0, 'step': 952}\n","[INFO|trainer_tf.py:595] 2021-03-19 22:39:02,840 >> Saving checkpoint for step 952 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased/checkpoint/ckpt-2\n","[INFO|trainer_tf.py:306] 2021-03-19 22:40:44,502 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:40:44,504 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 22:40:44,505 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 22:40:44,506 >>   Batch size = 32\n","[INFO|trainer_tf.py:404] 2021-03-19 22:40:49,126 >> {'eval_loss': 0.13496039807796478, 'eval_acc': 0.9541015625, 'epoch': 3.0, 'step': 1428}\n","[INFO|trainer_tf.py:404] 2021-03-19 22:40:49,133 >> {'loss': 0.16684313, 'learning_rate': 0.0, 'epoch': 3.0, 'step': 1428}\n","[INFO|trainer_tf.py:595] 2021-03-19 22:40:52,203 >> Saving checkpoint for step 1428 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased/checkpoint/ckpt-3\n","[INFO|trainer_tf.py:610] 2021-03-19 22:40:52,221 >> Training took: 0:06:17.738940\n","[INFO|trainer_tf.py:785] 2021-03-19 22:40:52,222 >> Saving model in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased\n","[INFO|configuration_utils.py:314] 2021-03-19 22:40:52,234 >> Configuration saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased/config.json\n","[INFO|modeling_tf_utils.py:1045] 2021-03-19 22:40:53,285 >> Model weights saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased/tf_model.h5\n","[INFO|tokenization_utils_base.py:1896] 2021-03-19 22:40:53,294 >> tokenizer config file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-03-19 22:40:53,305 >> Special tokens file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_distilbert-base-uncased/special_tokens_map.json\n","03/19/2021 22:40:53 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer_tf.py:306] 2021-03-19 22:40:53,357 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:40:53,358 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 22:40:53,359 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 22:40:53,366 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:40:53 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:40:53 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 22:40:58,864 >> {'eval_loss': 0.13496039807796478, 'eval_acc': 0.9541015625, 'epoch': 3.0, 'step': 1428}\n","03/19/2021 22:40:58 - INFO - __main__ -   ***** Eval results *****\n","03/19/2021 22:40:58 - INFO - __main__ -     eval_loss = 0.13496039807796478\n","03/19/2021 22:40:58 - INFO - __main__ -     eval_acc = 0.9541015625\n","03/19/2021 22:40:58 - INFO - __main__ -   *** predictions ***\n","[INFO|trainer_tf.py:306] 2021-03-19 22:40:58,891 >> ***** Running Prediction *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:40:58,896 >>   Num examples in dataset = 3263\n","[INFO|trainer_tf.py:310] 2021-03-19 22:40:58,899 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["a0954\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:41:13 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:41:13 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","03/19/2021 22:41:14 - INFO - __main__ -   *** RESUTS: ***\n","03/19/2021 22:41:14 - INFO - __main__ -   PredictionOutput(predictions=array([[-0.9648697,  1.1234663],\n","       [-2.4418776,  2.7539551],\n","       [-2.1869729,  2.5227158],\n","       ...,\n","       [-2.8132024,  2.9213922],\n","       [-1.3580459,  1.590666 ],\n","       [-2.6961486,  2.908961 ]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 0]), metrics={'eval_loss': 1.7891193464690565, 'eval_acc': 0.6019000919399325})\n","03/19/2021 22:41:14 - INFO - __main__ -   *** :RESUTS ***\n","03/19/2021 22:41:14 - INFO - __main__ -   removing checkpoints...\n","03/19/2021 22:41:14 - INFO - __main__ -   removing file tf_model.h5\n"],"name":"stderr"},{"output_type":"stream","text":["*******************\n","*** Processing: ***\n","*******************\n","MODEL NAME: roberta-base\n","RESULTS NAME: roberta-base_3e_v1024o_210319_2241\n","TIMESTAMP: 210319_2241\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|training_args.py:631] 2021-03-19 22:41:15,231 >> PyTorch: setting up devices\n","[INFO|training_args.py:555] 2021-03-19 22:41:15,234 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","[INFO|training_args_tf.py:192] 2021-03-19 22:41:15,238 >> Tensorflow: setting up strategy\n","03/19/2021 22:41:15 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n","03/19/2021 22:41:15 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Mar19_22-41-15_a7d129682a59', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=476, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=476, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=476, dataloader_num_workers=0, past_index=-1, run_name='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, tpu_name=None, tpu_zone=None, gcp_project=None, poly_power=1.0, xla=False)\n"],"name":"stderr"},{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|configuration_utils.py:463] 2021-03-19 22:41:15,456 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:499] 2021-03-19 22:41:15,457 >> Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.4.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:41:16,717 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:41:16,718 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:41:16,719 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:41:16,723 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:41:16,725 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:41:16,726 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","03/19/2021 22:41:17 - WARNING - datasets.builder -   Using custom data configuration default-829a6b88a52ac715\n","03/19/2021 22:41:17 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n","03/19/2021 22:41:17 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-a7ff06ecc0f59e87.arrow\n","03/19/2021 22:41:17 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-24db9ad23c6bf8c8.arrow\n","03/19/2021 22:41:17 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-48a850299f15b6bf.arrow\n"],"name":"stderr"},{"output_type":"stream","text":["Features Name: ['target', 'text']\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|configuration_utils.py:463] 2021-03-19 22:41:17,540 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","[INFO|configuration_utils.py:499] 2021-03-19 22:41:17,541 >> Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"finetuning_task\": \"text-classification\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.4.2\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|modeling_tf_utils.py:1240] 2021-03-19 22:41:17,752 >> loading weights file https://huggingface.co/roberta-base/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/22fef2e3c5012c1a8f8d7f024e30275dd2925b076abb5131dc3d1068345b6426.d409db346b0c1408865b9785d36744ccb988186626309ae8f995f86511811602.h5\n","[WARNING|modeling_tf_utils.py:1298] 2021-03-19 22:41:32,252 >> All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n","\n","[WARNING|modeling_tf_utils.py:1302] 2021-03-19 22:41:32,253 >> Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|trainer_tf.py:117] 2021-03-19 22:41:32,267 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n","[INFO|trainer_tf.py:125] 2021-03-19 22:41:32,268 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n","[INFO|trainer_tf.py:528] 2021-03-19 22:41:32,288 >> ***** Running training *****\n","[INFO|trainer_tf.py:529] 2021-03-19 22:41:32,288 >>   Num examples = 7613\n","[INFO|trainer_tf.py:531] 2021-03-19 22:41:32,289 >>   Num Epochs = 3\n","[INFO|trainer_tf.py:532] 2021-03-19 22:41:32,296 >>   Instantaneous batch size per device = 16\n","[INFO|trainer_tf.py:534] 2021-03-19 22:41:32,297 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer_tf.py:536] 2021-03-19 22:41:32,301 >>   Gradient Accumulation steps = 1\n","[INFO|trainer_tf.py:537] 2021-03-19 22:41:32,302 >>   Steps per epoch = 476\n","[INFO|trainer_tf.py:538] 2021-03-19 22:41:32,305 >>   Total optimization steps = 1428\n"],"name":"stderr"},{"output_type":"stream","text":["train: 7613\n","valid: 1024\n","test: 3263\n","Model name: roberta-base\n","Results name: roberta-base_3e_v1024o_210319_2241\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:41:35 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:41:35 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:41:41 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:41:41 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:306] 2021-03-19 22:45:04,203 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:45:04,206 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 22:45:04,207 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 22:45:04,208 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:45:04 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:45:04 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 22:45:13,973 >> {'eval_loss': 0.36592695116996765, 'eval_acc': 0.859375, 'epoch': 1.0, 'step': 476}\n","[INFO|trainer_tf.py:404] 2021-03-19 22:45:13,982 >> {'loss': 0.4499769, 'learning_rate': 3.333333e-05, 'epoch': 1.0, 'step': 476}\n","[INFO|trainer_tf.py:595] 2021-03-19 22:45:22,687 >> Saving checkpoint for step 476 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/checkpoint/ckpt-1\n","[INFO|trainer_tf.py:306] 2021-03-19 22:48:42,890 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:48:42,891 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 22:48:42,892 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 22:48:42,893 >>   Batch size = 32\n","[INFO|trainer_tf.py:404] 2021-03-19 22:48:51,210 >> {'eval_loss': 0.2956276535987854, 'eval_acc': 0.8857421875, 'epoch': 2.0, 'step': 952}\n","[INFO|trainer_tf.py:404] 2021-03-19 22:48:51,220 >> {'loss': 0.33803, 'learning_rate': 1.6666665e-05, 'epoch': 2.0, 'step': 952}\n","[INFO|trainer_tf.py:595] 2021-03-19 22:49:03,141 >> Saving checkpoint for step 952 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/checkpoint/ckpt-2\n","[INFO|trainer_tf.py:306] 2021-03-19 22:52:23,174 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:52:23,176 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 22:52:23,177 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 22:52:23,178 >>   Batch size = 32\n","[INFO|trainer_tf.py:404] 2021-03-19 22:52:31,368 >> {'eval_loss': 0.21554146707057953, 'eval_acc': 0.923828125, 'epoch': 3.0, 'step': 1428}\n","[INFO|trainer_tf.py:404] 2021-03-19 22:52:31,376 >> {'loss': 0.25989333, 'learning_rate': 0.0, 'epoch': 3.0, 'step': 1428}\n","[INFO|trainer_tf.py:595] 2021-03-19 22:52:39,818 >> Saving checkpoint for step 1428 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/checkpoint/ckpt-3\n","[INFO|trainer_tf.py:610] 2021-03-19 22:52:39,838 >> Training took: 0:11:07.523847\n","[INFO|trainer_tf.py:785] 2021-03-19 22:52:39,839 >> Saving model in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base\n","[INFO|configuration_utils.py:314] 2021-03-19 22:52:39,847 >> Configuration saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/config.json\n","[INFO|modeling_tf_utils.py:1045] 2021-03-19 22:52:51,377 >> Model weights saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/tf_model.h5\n","[INFO|tokenization_utils_base.py:1896] 2021-03-19 22:52:51,386 >> tokenizer config file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-03-19 22:52:51,466 >> Special tokens file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_roberta-base/special_tokens_map.json\n","03/19/2021 22:52:51 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer_tf.py:306] 2021-03-19 22:52:51,584 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:52:51,588 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 22:52:51,589 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 22:52:51,593 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:52:51 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:52:51 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 22:53:03,869 >> {'eval_loss': 0.21554146707057953, 'eval_acc': 0.923828125, 'epoch': 3.0, 'step': 1428}\n","03/19/2021 22:53:03 - INFO - __main__ -   ***** Eval results *****\n","03/19/2021 22:53:03 - INFO - __main__ -     eval_loss = 0.21554146707057953\n","03/19/2021 22:53:03 - INFO - __main__ -     eval_acc = 0.923828125\n","03/19/2021 22:53:03 - INFO - __main__ -   *** predictions ***\n","[INFO|trainer_tf.py:306] 2021-03-19 22:53:03,901 >> ***** Running Prediction *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:53:03,903 >>   Num examples in dataset = 3263\n","[INFO|trainer_tf.py:310] 2021-03-19 22:53:03,906 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["a0924\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:53:28 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:53:28 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","03/19/2021 22:53:30 - INFO - __main__ -   *** RESUTS: ***\n","03/19/2021 22:53:30 - INFO - __main__ -   PredictionOutput(predictions=array([[-2.242197 ,  2.0956461],\n","       [-1.7960083,  1.6218038],\n","       [-2.0303984,  1.8653785],\n","       ...,\n","       [-2.7090876,  2.6523266],\n","       [-0.9058579,  1.0671012],\n","       [-1.9163721,  1.7595011]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 0]), metrics={'eval_loss': 1.6623221004710478, 'eval_acc': 0.5758504443763408})\n","03/19/2021 22:53:30 - INFO - __main__ -   *** :RESUTS ***\n","03/19/2021 22:53:30 - INFO - __main__ -   removing checkpoints...\n","03/19/2021 22:53:31 - INFO - __main__ -   removing file tf_model.h5\n"],"name":"stderr"},{"output_type":"stream","text":["*******************\n","*** Processing: ***\n","*******************\n","MODEL NAME: bert-base-uncased\n","RESULTS NAME: bert-base-uncased_3e_v1024o_210319_2253\n","TIMESTAMP: 210319_2253\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|training_args.py:631] 2021-03-19 22:53:32,304 >> PyTorch: setting up devices\n","[INFO|training_args.py:555] 2021-03-19 22:53:32,309 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","[INFO|training_args_tf.py:192] 2021-03-19 22:53:32,316 >> Tensorflow: setting up strategy\n","03/19/2021 22:53:32 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n","03/19/2021 22:53:32 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Mar19_22-53-32_a7d129682a59', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=476, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=476, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=476, dataloader_num_workers=0, past_index=-1, run_name='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, tpu_name=None, tpu_zone=None, gcp_project=None, poly_power=1.0, xla=False)\n"],"name":"stderr"},{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|configuration_utils.py:463] 2021-03-19 22:53:32,543 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n","[INFO|configuration_utils.py:499] 2021-03-19 22:53:32,545 >> Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.4.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:53:33,588 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:53:33,589 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:53:33,590 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:53:33,591 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 22:53:33,592 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","03/19/2021 22:53:33 - WARNING - datasets.builder -   Using custom data configuration default-829a6b88a52ac715\n","03/19/2021 22:53:33 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n","03/19/2021 22:53:33 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-92c5e1e1e21bad80.arrow\n","03/19/2021 22:53:34 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-a7bf6948d616091c.arrow\n","03/19/2021 22:53:34 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-f74b44fd62d9bb67.arrow\n"],"name":"stderr"},{"output_type":"stream","text":["Features Name: ['target', 'text']\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|configuration_utils.py:463] 2021-03-19 22:53:34,411 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.637c6035640bacb831febcc2b7f7bee0a96f9b30c2d7e9ef84082d9f252f3170\n","[INFO|configuration_utils.py:499] 2021-03-19 22:53:34,413 >> Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"text-classification\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.4.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|modeling_tf_utils.py:1240] 2021-03-19 22:53:34,626 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/775efbdc2152093295bc5824dee96da82a5f3c1f218dfface1b8cef3094bdf8f.c719a806caef7d36ec0185f14b3b5fa727d919f924abe35622b4b7147bfbb8c7.h5\n","[WARNING|modeling_tf_utils.py:1298] 2021-03-19 22:53:47,349 >> All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","[WARNING|modeling_tf_utils.py:1302] 2021-03-19 22:53:47,350 >> Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|trainer_tf.py:117] 2021-03-19 22:53:47,360 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n","[INFO|trainer_tf.py:125] 2021-03-19 22:53:47,361 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n","[INFO|trainer_tf.py:528] 2021-03-19 22:53:47,392 >> ***** Running training *****\n","[INFO|trainer_tf.py:529] 2021-03-19 22:53:47,393 >>   Num examples = 7613\n","[INFO|trainer_tf.py:531] 2021-03-19 22:53:47,395 >>   Num Epochs = 3\n","[INFO|trainer_tf.py:532] 2021-03-19 22:53:47,401 >>   Instantaneous batch size per device = 16\n","[INFO|trainer_tf.py:534] 2021-03-19 22:53:47,402 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer_tf.py:536] 2021-03-19 22:53:47,403 >>   Gradient Accumulation steps = 1\n","[INFO|trainer_tf.py:537] 2021-03-19 22:53:47,404 >>   Steps per epoch = 476\n","[INFO|trainer_tf.py:538] 2021-03-19 22:53:47,405 >>   Total optimization steps = 1428\n"],"name":"stderr"},{"output_type":"stream","text":["train: 7613\n","valid: 1024\n","test: 3263\n","Model name: bert-base-uncased\n","Results name: bert-base-uncased_3e_v1024o_210319_2253\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:53:51 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:53:51 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:53:57 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:53:57 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:306] 2021-03-19 22:57:10,484 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 22:57:10,490 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 22:57:10,491 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 22:57:10,492 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:57:10 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 22:57:10 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 22:57:20,746 >> {'eval_loss': 0.3039873540401459, 'eval_acc': 0.8857421875, 'epoch': 1.0, 'step': 476}\n","[INFO|trainer_tf.py:404] 2021-03-19 22:57:20,754 >> {'loss': 0.41882545, 'learning_rate': 3.333333e-05, 'epoch': 1.0, 'step': 476}\n","[INFO|trainer_tf.py:595] 2021-03-19 22:57:26,513 >> Saving checkpoint for step 476 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased/checkpoint/ckpt-1\n","[INFO|trainer_tf.py:306] 2021-03-19 23:00:39,621 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 23:00:39,622 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 23:00:39,623 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 23:00:39,624 >>   Batch size = 32\n","[INFO|trainer_tf.py:404] 2021-03-19 23:00:48,536 >> {'eval_loss': 0.19302938878536224, 'eval_acc': 0.9326171875, 'epoch': 2.0, 'step': 952}\n","[INFO|trainer_tf.py:404] 2021-03-19 23:00:48,545 >> {'loss': 0.28175133, 'learning_rate': 1.6666665e-05, 'epoch': 2.0, 'step': 952}\n","[INFO|trainer_tf.py:595] 2021-03-19 23:00:55,253 >> Saving checkpoint for step 952 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased/checkpoint/ckpt-2\n","[INFO|trainer_tf.py:306] 2021-03-19 23:04:08,684 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 23:04:08,686 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 23:04:08,687 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 23:04:08,688 >>   Batch size = 32\n","[INFO|trainer_tf.py:404] 2021-03-19 23:04:17,653 >> {'eval_loss': 0.11916890740394592, 'eval_acc': 0.9609375, 'epoch': 3.0, 'step': 1428}\n","[INFO|trainer_tf.py:404] 2021-03-19 23:04:17,661 >> {'loss': 0.17749618, 'learning_rate': 0.0, 'epoch': 3.0, 'step': 1428}\n","[INFO|trainer_tf.py:595] 2021-03-19 23:04:23,815 >> Saving checkpoint for step 1428 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased/checkpoint/ckpt-3\n","[INFO|trainer_tf.py:610] 2021-03-19 23:04:23,836 >> Training took: 0:10:36.420736\n","[INFO|trainer_tf.py:785] 2021-03-19 23:04:23,837 >> Saving model in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased\n","[INFO|configuration_utils.py:314] 2021-03-19 23:04:23,848 >> Configuration saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased/config.json\n","[INFO|modeling_tf_utils.py:1045] 2021-03-19 23:04:35,503 >> Model weights saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased/tf_model.h5\n","[INFO|tokenization_utils_base.py:1896] 2021-03-19 23:04:35,527 >> tokenizer config file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-03-19 23:04:35,551 >> Special tokens file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-uncased/special_tokens_map.json\n","03/19/2021 23:04:35 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer_tf.py:306] 2021-03-19 23:04:35,711 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 23:04:35,712 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 23:04:35,716 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 23:04:35,718 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:04:36 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:04:36 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 23:04:46,630 >> {'eval_loss': 0.11916890740394592, 'eval_acc': 0.9609375, 'epoch': 3.0, 'step': 1428}\n","03/19/2021 23:04:46 - INFO - __main__ -   ***** Eval results *****\n","03/19/2021 23:04:46 - INFO - __main__ -     eval_loss = 0.11916890740394592\n","03/19/2021 23:04:46 - INFO - __main__ -     eval_acc = 0.9609375\n","03/19/2021 23:04:46 - INFO - __main__ -   *** predictions ***\n","[INFO|trainer_tf.py:306] 2021-03-19 23:04:46,656 >> ***** Running Prediction *****\n","[INFO|trainer_tf.py:307] 2021-03-19 23:04:46,659 >>   Num examples in dataset = 3263\n","[INFO|trainer_tf.py:310] 2021-03-19 23:04:46,660 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["a0961\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:05:14 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:05:14 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","03/19/2021 23:05:15 - INFO - __main__ -   *** RESUTS: ***\n","03/19/2021 23:05:15 - INFO - __main__ -   PredictionOutput(predictions=array([[-1.791618 ,  2.1779225],\n","       [-1.9666513,  2.3399668],\n","       [-1.6043497,  2.0729783],\n","       ...,\n","       [-2.4841366,  2.6329815],\n","       [-2.0199013,  2.3375275],\n","       [-2.1570654,  2.3207397]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 0]), metrics={'eval_loss': 1.7081491806927849, 'eval_acc': 0.5887220349371743})\n","03/19/2021 23:05:15 - INFO - __main__ -   *** :RESUTS ***\n","03/19/2021 23:05:16 - INFO - __main__ -   removing checkpoints...\n","03/19/2021 23:05:16 - INFO - __main__ -   removing file tf_model.h5\n"],"name":"stderr"},{"output_type":"stream","text":["*******************\n","*** Processing: ***\n","*******************\n","MODEL NAME: bert-base-multilingual-uncased\n","RESULTS NAME: bert-base-multilingual-uncased_3e_v1024o_210319_2305\n","TIMESTAMP: 210319_2305\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|training_args.py:631] 2021-03-19 23:05:17,664 >> PyTorch: setting up devices\n","[INFO|training_args.py:555] 2021-03-19 23:05:17,674 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","[INFO|training_args_tf.py:192] 2021-03-19 23:05:17,683 >> Tensorflow: setting up strategy\n","03/19/2021 23:05:17 - INFO - __main__ -   n_replicas: 1, distributed training: False, 16-bits training: False\n","03/19/2021 23:05:17 - INFO - __main__ -   Training/evaluation parameters TFTrainingArguments(output_dir='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased', overwrite_output_dir=True, do_train=True, do_eval=True, do_predict=True, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=32, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, logging_dir='runs/Mar19_23-05-17_a7d129682a59', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=476, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=476, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', fp16_backend='auto', fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=476, dataloader_num_workers=0, past_index=-1, run_name='/content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, tpu_name=None, tpu_zone=None, gcp_project=None, poly_power=1.0, xla=False)\n"],"name":"stderr"},{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"},{"output_type":"stream","text":["[INFO|configuration_utils.py:463] 2021-03-19 23:05:17,922 >> loading configuration file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/af4e101d208f361f141144dca21e9c4148aaf0e85441c2e335743d10829c6cad.d63adade93e44e64bedd306ec82ffd33eedabaf0ff08aabe581acaa48616a508\n","[INFO|configuration_utils.py:499] 2021-03-19 23:05:17,923 >> Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.4.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 105879\n","}\n","\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 23:05:18,991 >> loading file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/269f2943d168a4cd2ddf3864cee89d7f7d78873b3d14a1229174d37212981a38.92022aa29ab6663b0b4254744f28ab43e6adf4deebe0f26651e6c61f28f69d8b\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 23:05:18,992 >> loading file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/857db185d48b92f3e6141ef5092d8d5dbebab7eef1bacc6c9eaf85cf23807641.73ad1f9fd9f94089672128003fb4a687b64b73b2bfb8d08766bbc71feec8cd96\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 23:05:18,993 >> loading file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 23:05:18,998 >> loading file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1702] 2021-03-19 23:05:19,001 >> loading file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/1b935b135ddb021a7d836c00f5702b80d11d348fd5c5a42cbd933d8ed1f55be9.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n","03/19/2021 23:05:19 - WARNING - datasets.builder -   Using custom data configuration default-829a6b88a52ac715\n","03/19/2021 23:05:19 - WARNING - datasets.builder -   Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0)\n","03/19/2021 23:05:19 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-7804bf3820ff2254.arrow\n","03/19/2021 23:05:19 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-c08884c9cf84722d.arrow\n"],"name":"stderr"},{"output_type":"stream","text":["Features Name: ['target', 'text']\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:05:19 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-829a6b88a52ac715/0.0.0/2dc6629a9ff6b5697d82c25b73731dd440507a69cbce8b425db50b751e8fcfd0/cache-c3413423eb42b5a1.arrow\n","[INFO|configuration_utils.py:463] 2021-03-19 23:05:19,933 >> loading configuration file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/af4e101d208f361f141144dca21e9c4148aaf0e85441c2e335743d10829c6cad.d63adade93e44e64bedd306ec82ffd33eedabaf0ff08aabe581acaa48616a508\n","[INFO|configuration_utils.py:499] 2021-03-19 23:05:19,934 >> Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"directionality\": \"bidi\",\n","  \"finetuning_task\": \"text-classification\",\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"0\": 0,\n","    \"1\": 1\n","  },\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.4.2\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 105879\n","}\n","\n","[INFO|modeling_tf_utils.py:1240] 2021-03-19 23:05:20,142 >> loading weights file https://huggingface.co/bert-base-multilingual-uncased/resolve/main/tf_model.h5 from cache at /root/.cache/huggingface/transformers/b359fd4f27130bf933d9dc99f97cdbb7580d8189d23f256237a0302b4cd0f6ba.a9fe9a746494a04923c8d64e009d09a3c30200e462dd8b7a98726947f15f682c.h5\n","[WARNING|modeling_tf_utils.py:1298] 2021-03-19 23:05:39,363 >> All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","[WARNING|modeling_tf_utils.py:1302] 2021-03-19 23:05:39,364 >> Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","[INFO|trainer_tf.py:117] 2021-03-19 23:05:39,374 >> You are instantiating a Trainer but W&B is not installed. To use wandb logging, run `pip install wandb; wandb login` see https://docs.wandb.com/huggingface.\n","[INFO|trainer_tf.py:125] 2021-03-19 23:05:39,375 >> To use comet_ml logging, run `pip/conda install comet_ml` see https://www.comet.ml/docs/python-sdk/huggingface/\n","[INFO|trainer_tf.py:528] 2021-03-19 23:05:39,405 >> ***** Running training *****\n","[INFO|trainer_tf.py:529] 2021-03-19 23:05:39,406 >>   Num examples = 7613\n","[INFO|trainer_tf.py:531] 2021-03-19 23:05:39,410 >>   Num Epochs = 3\n","[INFO|trainer_tf.py:532] 2021-03-19 23:05:39,413 >>   Instantaneous batch size per device = 16\n","[INFO|trainer_tf.py:534] 2021-03-19 23:05:39,413 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer_tf.py:536] 2021-03-19 23:05:39,414 >>   Gradient Accumulation steps = 1\n","[INFO|trainer_tf.py:537] 2021-03-19 23:05:39,415 >>   Steps per epoch = 476\n","[INFO|trainer_tf.py:538] 2021-03-19 23:05:39,417 >>   Total optimization steps = 1428\n"],"name":"stderr"},{"output_type":"stream","text":["train: 7613\n","valid: 1024\n","test: 3263\n","Model name: bert-base-multilingual-uncased\n","Results name: bert-base-multilingual-uncased_3e_v1024o_210319_2305\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:05:43 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:05:43 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:05:48 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:05:48 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:306] 2021-03-19 23:09:15,199 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 23:09:15,200 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 23:09:15,202 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 23:09:15,203 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:09:15 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:09:15 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 23:09:25,532 >> {'eval_loss': 0.3208419680595398, 'eval_acc': 0.8642578125, 'epoch': 1.0, 'step': 476}\n","[INFO|trainer_tf.py:404] 2021-03-19 23:09:25,539 >> {'loss': 0.4490564, 'learning_rate': 3.333333e-05, 'epoch': 1.0, 'step': 476}\n","[INFO|trainer_tf.py:595] 2021-03-19 23:09:46,095 >> Saving checkpoint for step 476 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased/checkpoint/ckpt-1\n","[INFO|trainer_tf.py:306] 2021-03-19 23:13:12,564 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 23:13:12,566 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 23:13:12,567 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 23:13:12,568 >>   Batch size = 32\n","[INFO|trainer_tf.py:404] 2021-03-19 23:13:21,483 >> {'eval_loss': 0.22632597386837006, 'eval_acc': 0.9169921875, 'epoch': 2.0, 'step': 952}\n","[INFO|trainer_tf.py:404] 2021-03-19 23:13:21,492 >> {'loss': 0.31967455, 'learning_rate': 1.6666665e-05, 'epoch': 2.0, 'step': 952}\n","[INFO|trainer_tf.py:595] 2021-03-19 23:13:38,245 >> Saving checkpoint for step 952 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased/checkpoint/ckpt-2\n","[INFO|trainer_tf.py:306] 2021-03-19 23:17:04,442 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 23:17:04,445 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 23:17:04,446 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 23:17:04,447 >>   Batch size = 32\n","[INFO|trainer_tf.py:404] 2021-03-19 23:17:13,321 >> {'eval_loss': 0.1612837016582489, 'eval_acc': 0.9453125, 'epoch': 3.0, 'step': 1428}\n","[INFO|trainer_tf.py:404] 2021-03-19 23:17:13,330 >> {'loss': 0.2227093, 'learning_rate': 0.0, 'epoch': 3.0, 'step': 1428}\n","[INFO|trainer_tf.py:595] 2021-03-19 23:17:33,091 >> Saving checkpoint for step 1428 at /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased/checkpoint/ckpt-3\n","[INFO|trainer_tf.py:610] 2021-03-19 23:17:33,115 >> Training took: 0:11:53.681262\n","[INFO|trainer_tf.py:785] 2021-03-19 23:17:33,116 >> Saving model in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased\n","[INFO|configuration_utils.py:314] 2021-03-19 23:17:33,139 >> Configuration saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased/config.json\n","[INFO|modeling_tf_utils.py:1045] 2021-03-19 23:17:47,568 >> Model weights saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased/tf_model.h5\n","[INFO|tokenization_utils_base.py:1896] 2021-03-19 23:17:50,436 >> tokenizer config file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:1902] 2021-03-19 23:17:50,441 >> Special tokens file saved in /content/gdrive/MyDrive/Colab Notebooks/data/disaster_tweets/mod_bert-base-multilingual-uncased/special_tokens_map.json\n","03/19/2021 23:17:50 - INFO - __main__ -   *** Evaluate ***\n","[INFO|trainer_tf.py:306] 2021-03-19 23:17:50,648 >> ***** Running Evaluation *****\n","[INFO|trainer_tf.py:307] 2021-03-19 23:17:50,649 >>   Num examples in dataset = 1024\n","[INFO|trainer_tf.py:309] 2021-03-19 23:17:50,651 >>   Num examples in used in evaluation = 1024\n","[INFO|trainer_tf.py:310] 2021-03-19 23:17:50,656 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:17:50 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:17:50 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","[INFO|trainer_tf.py:404] 2021-03-19 23:18:00,548 >> {'eval_loss': 0.1612837016582489, 'eval_acc': 0.9453125, 'epoch': 3.0, 'step': 1428}\n","03/19/2021 23:18:00 - INFO - __main__ -   ***** Eval results *****\n","03/19/2021 23:18:00 - INFO - __main__ -     eval_loss = 0.1612837016582489\n","03/19/2021 23:18:00 - INFO - __main__ -     eval_acc = 0.9453125\n","03/19/2021 23:18:00 - INFO - __main__ -   *** predictions ***\n","[INFO|trainer_tf.py:306] 2021-03-19 23:18:00,573 >> ***** Running Prediction *****\n","[INFO|trainer_tf.py:307] 2021-03-19 23:18:00,576 >>   Num examples in dataset = 3263\n","[INFO|trainer_tf.py:310] 2021-03-19 23:18:00,579 >>   Batch size = 32\n"],"name":"stderr"},{"output_type":"stream","text":["a0945\n","WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:18:27 - WARNING - tensorflow -   The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"],"name":"stdout"},{"output_type":"stream","text":["03/19/2021 23:18:27 - WARNING - tensorflow -   The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","03/19/2021 23:18:29 - INFO - __main__ -   *** RESUTS: ***\n","03/19/2021 23:18:29 - INFO - __main__ -   PredictionOutput(predictions=array([[-1.3270354,  1.4175619],\n","       [-2.5180933,  2.3808997],\n","       [-2.4241726,  2.336313 ],\n","       ...,\n","       [-2.8179977,  2.4849398],\n","       [-0.9030537,  1.0315692],\n","       [-2.525087 ,  2.4081502]], dtype=float32), label_ids=array([0, 0, 0, ..., 0, 0, 0]), metrics={'eval_loss': 1.6473717783011643, 'eval_acc': 0.5985289610787619})\n","03/19/2021 23:18:29 - INFO - __main__ -   *** :RESUTS ***\n","03/19/2021 23:18:29 - INFO - __main__ -   removing checkpoints...\n","03/19/2021 23:18:29 - INFO - __main__ -   removing file tf_model.h5\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"8NkE82kJmPLP","executionInfo":{"status":"ok","timestamp":1616195910566,"user_tz":-60,"elapsed":2658566,"user":{"displayName":"Jarek","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg3DcpCKL3sgVbZ4G2ARjJ5H7Bed_osRirjB8lz=s64","userId":"13173705756794122601"}}},"source":[""],"execution_count":9,"outputs":[]}]}